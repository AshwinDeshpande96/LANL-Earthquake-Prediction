{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LANL_Earthquake.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "RcRbSXeS_0ye",
        "poKFNqLCAmoo"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AshwinDeshpande96/LANL-Earthquake-Prediction/blob/master/LANL_Earthquake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcRbSXeS_0ye",
        "colab_type": "text"
      },
      "source": [
        "# Raw Data Fetching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAN_GVUPkTNi",
        "colab_type": "code",
        "outputId": "1e9bdf88-2647-43a2-f293-c30dd5613375",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#Get 'kaggle.json' file from your kaggle profile. Click on 'Edit Profile' --> 'Create New API Token'\n",
        "#Upload 'kaggle.json' file\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-afb23514-9767-40ae-8d25-3a3619511b05\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-afb23514-9767-40ae-8d25-3a3619511b05\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRFotDm5BsqT",
        "colab_type": "code",
        "outputId": "89a2bcfd-05ff-4679-e2d3-e5bae4261b84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#Make /root/.kaggle/ directory. So that kaggle command can download the data\n",
        "!mkdir /root/.kaggle/\n",
        "#Move downloaded 'kaggle.json' to previously created directory\n",
        "!mv /content/kaggle.json /root/.kaggle/\n",
        "#change permissions of 'kaggle.json' so that it is only accessible by you\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!ls /content/"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle/’: File exists\n",
            "mv: cannot stat '/content/kaggle.json': No such file or directory\n",
            "gdrive\tsample_data  sample_submission.csv  test.zip  train.csv.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7V5-bEIjdUm",
        "colab_type": "code",
        "outputId": "6783656f-ddc6-423a-c2bd-0291c7c2b8b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "#download the Data given by Kaggle Competition\n",
        "!kaggle competitions download -c LANL-Earthquake-Prediction\n",
        "#this downloads 3 files: \n",
        "#1) sample_submisison.csv  \n",
        "#2) test.zip - (this contains 2624 csv files with 150000 rows of acoustic_data)   \n",
        "#3) train.csv.zip - Contains one csv file with approx 150000*4195 rows of ['acoustic_data', 'time_to_failure']\n",
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading sample_submission.csv to /content\n",
            "\r  0% 0.00/33.3k [00:00<?, ?B/s]\n",
            "\r100% 33.3k/33.3k [00:00<00:00, 32.9MB/s]\n",
            "Downloading test.zip to /content\n",
            " 96% 233M/242M [00:02<00:00, 82.3MB/s]\n",
            "100% 242M/242M [00:02<00:00, 98.7MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            " 99% 2.01G/2.03G [00:17<00:00, 90.9MB/s]\n",
            "100% 2.03G/2.03G [00:17<00:00, 123MB/s] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa19xolvIsI7",
        "colab_type": "code",
        "outputId": "679e02ae-2d8a-416d-90b9-834bdc527909",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#extract train.csv.zip file to obtain train.csv file\n",
        "!unzip train.csv.zip\n",
        "!ls"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  train.csv.zip\n",
            "  inflating: train.csv               "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMjG7mqmhtbw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#extract test.zip file to obtain 2624 'seg_******.csv' files\n",
        "!unzip test.zip\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlOKckUt_83u",
        "colab_type": "text"
      },
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXKDWLMNsoVQ",
        "colab_type": "code",
        "outputId": "b287cd8b-49c5-4e48-90c1-b1be4b770654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#Link up to Google Drive to store Extracted Feature Data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP1t3FIHpsBf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "#this function returns a single feature: 'abs_trend'\n",
        "def add_trend_feature(arr, abs_values=False):\n",
        "    \"\"\"Fit a univariate linear regression and return the coefficient.\"\"\"\n",
        "    idx = np.array(range(len(arr)))\n",
        "    if abs_values:\n",
        "        arr = np.abs(arr)\n",
        "    lr = LinearRegression()\n",
        "    lr.fit(idx.reshape(-1, 1), arr)\n",
        "    return lr.coef_[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wC_4evncJsu_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this function creates a row  124 features: ['abs_max', 'abs_mean', 'abs_std', 'abs_trend', 'ave', 'ave_roll_abs_mean_10', 'ave_roll_abs_mean_100', 'ave_roll_abs_mean_1000', 'ave_roll_abs_mean_50', 'ave_roll_mean_10', \n",
        "#'ave_roll_mean_100', 'ave_roll_mean_1000', 'ave_roll_mean_50', 'ave_roll_std_10', 'ave_roll_std_100', 'ave_roll_std_1000', 'ave_roll_std_50', 'max', 'max_roll_mean_10', 'max_roll_mean_100', 'max_roll_mean_1000', \n",
        "#'max_roll_mean_50', 'max_roll_min_10', 'max_roll_min_100', 'max_roll_min_1000', 'max_roll_min_50', 'max_roll_std_10', 'max_roll_std_100', 'max_roll_std_1000', 'max_roll_std_50', 'min', 'min_roll_max_10', \n",
        "#'min_roll_max_100', 'min_roll_max_1000', 'min_roll_max_50', 'min_roll_mean_10', 'min_roll_mean_100', 'min_roll_mean_1000', 'min_roll_mean_50', 'min_roll_std_10', 'min_roll_std_100', 'min_roll_std_1000', \n",
        "#'min_roll_std_50', 'q01', 'q01_roll_std_10', 'q01_roll_std_100', 'q01_roll_std_1000', 'q01_roll_std_50', 'q05', 'q05_roll_abs_mean_10', 'q05_roll_abs_mean_100', 'q05_roll_abs_mean_1000', 'q05_roll_abs_mean_50', \n",
        "#'q05_roll_max_10', 'q05_roll_max_100', 'q05_roll_max_1000', 'q05_roll_max_50', 'q05_roll_mean_10', 'q05_roll_mean_100', 'q05_roll_mean_1000', 'q05_roll_mean_50', 'q05_roll_min_10', 'q05_roll_min_100', \n",
        "#'q05_roll_min_1000', 'q05_roll_min_50', 'q05_roll_std_10', 'q05_roll_std_100', 'q05_roll_std_1000', 'q05_roll_std_50', 'q10', 'q10_roll_std_10', 'q10_roll_std_100', 'q10_roll_std_1000', 'q10_roll_std_50', 'q90', 'q95',\n",
        "#'q95_roll_abs_mean_10', 'q95_roll_abs_mean_100', 'q95_roll_abs_mean_1000', 'q95_roll_abs_mean_50', 'q95_roll_max_10', 'q95_roll_max_100', 'q95_roll_max_1000', 'q95_roll_max_50', 'q95_roll_mean_10', 'q95_roll_mean_100',\n",
        "#'q95_roll_mean_1000', 'q95_roll_mean_50', 'q95_roll_min_10', 'q95_roll_min_100', 'q95_roll_min_1000', 'q95_roll_min_50', 'q95_roll_std_10', 'q95_roll_std_100', 'q95_roll_std_1000', 'q95_roll_std_50', 'q99', \n",
        "#'q99_roll_std_10', 'q99_roll_std_100', 'q99_roll_std_1000', 'q99_roll_std_50', 'std', 'std_roll_abs_mean_10', 'std_roll_abs_mean_100', 'std_roll_abs_mean_1000', 'std_roll_abs_mean_50', 'std_roll_max_10', \n",
        "#'std_roll_max_100', 'std_roll_max_1000', 'std_roll_max_50', 'std_roll_mean_10', 'std_roll_mean_100', 'std_roll_mean_1000', 'std_roll_mean_50', 'std_roll_min_10', 'std_roll_min_100', 'std_roll_min_1000', \n",
        "#'std_roll_min_50', 'std_roll_std_10', 'std_roll_std_100', 'std_roll_std_1000', 'std_roll_std_50', 'std_to_mean', 'trend']\n",
        "def extract_features_from_segment(x):\n",
        "    \"\"\"Returns a dictionary with the features for the given segment of acoustic data.\"\"\"\n",
        "    features = {}\n",
        "    \n",
        "    features['ave'] = x.values.mean()\n",
        "    features['std'] = x.values.std()\n",
        "    features['max'] = x.values.max()\n",
        "    features['min'] = x.values.min()\n",
        "    features['q90'] = np.quantile(x.values, 0.90)\n",
        "    features['q95'] = np.quantile(x.values, 0.95)\n",
        "    features['q99'] = np.quantile(x.values, 0.99)\n",
        "    features['q05'] = np.quantile(x.values, 0.05)\n",
        "    features['q10'] = np.quantile(x.values, 0.10)\n",
        "    features['q01'] = np.quantile(x.values, 0.01)\n",
        "    features['std_to_mean'] = features['std'] / features['ave']\n",
        "    \n",
        "    features['abs_max'] = np.abs(x.values).max()\n",
        "    features['abs_mean'] = np.abs(x.values).mean()\n",
        "    features['abs_std'] = np.abs(x.values).std()\n",
        "    features['trend'] = add_trend_feature(x.values)\n",
        "    features['abs_trend'] = add_trend_feature(x.values, abs_values=True)\n",
        "    \n",
        "    # New features - rolling features\n",
        "    for w in [10, 50, 100, 1000]:\n",
        "        x_roll_abs_mean = x.abs().rolling(w).mean().dropna().values\n",
        "        x_roll_mean = x.rolling(w).mean().dropna().values\n",
        "        x_roll_std = x.rolling(w).std().dropna().values\n",
        "        x_roll_min = x.rolling(w).min().dropna().values\n",
        "        x_roll_max = x.rolling(w).max().dropna().values\n",
        "        \n",
        "        features['ave_roll_std_' + str(w)] = x_roll_std.mean()\n",
        "        features['std_roll_std_' + str(w)] = x_roll_std.std()\n",
        "        features['max_roll_std_' + str(w)] = x_roll_std.max()\n",
        "        features['min_roll_std_' + str(w)] = x_roll_std.min()\n",
        "        features['q01_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.01)\n",
        "        features['q05_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.05)\n",
        "        features['q10_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.10)\n",
        "        features['q95_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.95)\n",
        "        features['q99_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.99)\n",
        "        \n",
        "        features['ave_roll_mean_' + str(w)] = x_roll_mean.mean()\n",
        "        features['std_roll_mean_' + str(w)] = x_roll_mean.std()\n",
        "        features['max_roll_mean_' + str(w)] = x_roll_mean.max()\n",
        "        features['min_roll_mean_' + str(w)] = x_roll_mean.min()\n",
        "        features['q05_roll_mean_' + str(w)] = np.quantile(x_roll_mean, 0.05)\n",
        "        features['q95_roll_mean_' + str(w)] = np.quantile(x_roll_mean, 0.95)\n",
        "        \n",
        "        features['ave_roll_abs_mean_' + str(w)] = x_roll_abs_mean.mean()\n",
        "        features['std_roll_abs_mean_' + str(w)] = x_roll_abs_mean.std()\n",
        "        features['q05_roll_abs_mean_' + str(w)] = np.quantile(x_roll_abs_mean, 0.05)\n",
        "        features['q95_roll_abs_mean_' + str(w)] = np.quantile(x_roll_abs_mean, 0.95)\n",
        "        \n",
        "        features['std_roll_min_' + str(w)] = x_roll_min.std()\n",
        "        features['max_roll_min_' + str(w)] = x_roll_min.max()\n",
        "        features['q05_roll_min_' + str(w)] = np.quantile(x_roll_min, 0.05)\n",
        "        features['q95_roll_min_' + str(w)] = np.quantile(x_roll_min, 0.95)\n",
        "\n",
        "        features['std_roll_max_' + str(w)] = x_roll_max.std()\n",
        "        features['min_roll_max_' + str(w)] = x_roll_max.min()\n",
        "        features['q05_roll_max_' + str(w)] = np.quantile(x_roll_max, 0.05)\n",
        "        features['q95_roll_max_' + str(w)] = np.quantile(x_roll_max, 0.95)\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1L1HoL0lMLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import time as t\n",
        "import os\n",
        "#Feature Extraction found at https://www.kaggle.com/jsaguiar/baseline-with-multiple-models\n",
        "#make_train function returns a 3 things:\n",
        "#!) X_train - Training Set of shape (4195 * 124) - Each of 4195 rows is contructed by calling 'extract_features_from_segment' function from 150,000 data-chunk of train.csv file\n",
        "#2) target - target variable is the value of every 150,000th  train.csv's ['time_to_failure'] data\n",
        "#3) quake - not used in this project\n",
        "def make_train():\n",
        "    features_list = []\n",
        "    target_list = []\n",
        "    quake_num = []\n",
        "    quake_count = 0\n",
        "    row = 1\n",
        "    time_so_far = 0\n",
        "    for segment in pd.read_csv('/content/train.csv', chunksize=150000):\n",
        "        \n",
        "        start = t.time()\n",
        "        \n",
        "        target_list.append(segment['time_to_failure'].values[-1])\n",
        "        features_list.append(extract_features_from_segment(segment['acoustic_data']))\n",
        "        quake_num.append(quake_count)\n",
        "        \n",
        "        if any(segment['time_to_failure'].diff() > 5):\n",
        "            quake_count = quake_count + 1\n",
        "        \n",
        "        end = t.time()\n",
        "        \n",
        "        time_taken = end - start\n",
        "        time_so_far += time_taken\n",
        "        \n",
        "        print \"\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\", row, \"/4195 \\n ETA: \", round(time_so_far/60, 1), \"/\", int(time_taken*4195/60) \n",
        "        \n",
        "        row = row + 1\n",
        "    return pd.DataFrame(features_list), pd.Series(target_list), pd.Series(quake_num)\n",
        "#reads each 2624 'seg_******.csv' file and creates a (2624*124) dataset for testing\n",
        "def make_test():\n",
        "    features_list = {}\n",
        "    \n",
        "    row = 1\n",
        "    time_so_far = 0\n",
        "    test_files = [file for file in os.listdir(os.getcwd()) if file.startswith(\"seg\")]          #returns an array of every seg_******.csv file in current directory\n",
        "    num_test_file - len(test_files)\n",
        "    for file in test_files:\n",
        "        segment = pd.read_csv(file)\n",
        "        start = t.time()\n",
        "        \n",
        "        \n",
        "        features_list.append(extract_features_from_segment(segment['acoustic_data']))\n",
        "        \n",
        "        \n",
        "        end = t.time()\n",
        "        \n",
        "        time_taken = end - start\n",
        "        time_so_far += time_taken\n",
        "        \n",
        "        print \"\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\", row, \"\\b/\" +num_test_file+\"\\nETA: \", round(time_so_far/60, 1), \"\\b/\", int(time_taken*num_test_file/60) \n",
        "        \n",
        "        row = row + 1\n",
        "    return pd.DataFrame(features_list)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-pjQTbJpdB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X_train, target, quake is created\n",
        "X_train, target, quake = make_train()\n",
        "print \"Train shape:\", X_train.shape, \"\\n\", X_train.head(3), \"\\nTarget shape:\", target.shape, \"\\n\", target.head(3), \"\\nQuake shape:\", quake.shape, \"\\n\", quake.head(3), \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VJaGIYnt0mx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#files are saved onto Google Drive in csv format with X_train - (4195*124) rows & colums, target with (4195, 1) rows & columns, quake with (4195, 1) rows & columns\n",
        "#change filepath according to your drive directory\n",
        "X_train.to_csv(\"/content/gdrive/My Drive/Projects/Kaggle Challenges/LANL Earthquake Prediction/X_train.csv\", index=False)\n",
        "target.to_csv(\"/content/gdrive/My Drive/Projects/Kaggle Challenges/LANL Earthquake Prediction/target.csv\", index=False)\n",
        "quake.to_csv(\"/content/gdrive/My Drive/Projects/Kaggle Challenges/LANL Earthquake Prediction/quake.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiYnyNXT8gJg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test_set is created\n",
        "testset = make_test()\n",
        "#files are saved onto Google Drive in csv format with test_set.csv - (2624*124) rows & colums\n",
        "testset.to_csv(\"/content/gdrive/My Drive/Projects/Kaggle Challenges/LANL Earthquake Prediction/test_set.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaIvtWWVQkJg",
        "colab_type": "text"
      },
      "source": [
        "Once X-train.csv, target.csv, quake.csv, test_set.csv are created we do not have to run previous cells again. \n",
        "Further program uses only the data saved in csv file in Google drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0IeyaSMAGhT",
        "colab_type": "text"
      },
      "source": [
        "# Data Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbOiqzSBPcd8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We start by importing previously created csv files\n",
        "import pandas as pd\n",
        "X = pd.read_csv(\"/content/gdrive/My Drive/Projects/Kaggle Challenges/LANL Earthquake Prediction/X_train.csv\")\n",
        "Y = pd.read_csv(\"/content/gdrive/My Drive/Projects/Kaggle Challenges/LANL Earthquake Prediction/target.csv\")\n",
        "#Uncomment if u tend to use quake data\n",
        "#quake = pd.read_csv(\"/content/gdrive/My Drive/Projects/Kaggle Challenges/LANL Earthquake Prediction/quake.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZlhwtoVaSAs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "26821e60-155c-4f87-d088-0a6c725fc0c4"
      },
      "source": [
        "#X_train.drop(X_train.tail(1).index,inplace=True)\n",
        "print \"Training Data:\\t\", X.shape\n",
        "print \"  Target Data:\\t\", Y.shape\n",
        "\n",
        "#Display sample of training data\n",
        "X.head(5)\n"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Data:\t(4194, 124)\n",
            "  Target Data:\t(4194, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abs_max</th>\n",
              "      <th>abs_mean</th>\n",
              "      <th>abs_std</th>\n",
              "      <th>abs_trend</th>\n",
              "      <th>ave</th>\n",
              "      <th>ave_roll_abs_mean_10</th>\n",
              "      <th>ave_roll_abs_mean_100</th>\n",
              "      <th>ave_roll_abs_mean_1000</th>\n",
              "      <th>ave_roll_abs_mean_50</th>\n",
              "      <th>ave_roll_mean_10</th>\n",
              "      <th>...</th>\n",
              "      <th>std_roll_min_10</th>\n",
              "      <th>std_roll_min_100</th>\n",
              "      <th>std_roll_min_1000</th>\n",
              "      <th>std_roll_min_50</th>\n",
              "      <th>std_roll_std_10</th>\n",
              "      <th>std_roll_std_100</th>\n",
              "      <th>std_roll_std_1000</th>\n",
              "      <th>std_roll_std_50</th>\n",
              "      <th>std_to_mean</th>\n",
              "      <th>trend</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>104</td>\n",
              "      <td>5.576567</td>\n",
              "      <td>4.333310</td>\n",
              "      <td>-0.000011</td>\n",
              "      <td>4.884113</td>\n",
              "      <td>5.576566</td>\n",
              "      <td>5.576655</td>\n",
              "      <td>5.579118</td>\n",
              "      <td>5.576561</td>\n",
              "      <td>4.884088</td>\n",
              "      <td>...</td>\n",
              "      <td>4.526280</td>\n",
              "      <td>6.522304</td>\n",
              "      <td>11.248606</td>\n",
              "      <td>5.787738</td>\n",
              "      <td>2.809071</td>\n",
              "      <td>3.111524</td>\n",
              "      <td>2.769772</td>\n",
              "      <td>3.176148</td>\n",
              "      <td>1.044425</td>\n",
              "      <td>-3.268300e-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>181</td>\n",
              "      <td>5.734167</td>\n",
              "      <td>5.732758</td>\n",
              "      <td>-0.000005</td>\n",
              "      <td>4.725767</td>\n",
              "      <td>5.734187</td>\n",
              "      <td>5.734653</td>\n",
              "      <td>5.739510</td>\n",
              "      <td>5.734444</td>\n",
              "      <td>4.725732</td>\n",
              "      <td>...</td>\n",
              "      <td>6.398675</td>\n",
              "      <td>10.162552</td>\n",
              "      <td>21.096014</td>\n",
              "      <td>8.823748</td>\n",
              "      <td>4.120785</td>\n",
              "      <td>4.893431</td>\n",
              "      <td>4.492905</td>\n",
              "      <td>4.953472</td>\n",
              "      <td>1.394229</td>\n",
              "      <td>9.090424e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>140</td>\n",
              "      <td>6.152647</td>\n",
              "      <td>5.895925</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>4.906393</td>\n",
              "      <td>6.152557</td>\n",
              "      <td>6.153045</td>\n",
              "      <td>6.159850</td>\n",
              "      <td>6.152678</td>\n",
              "      <td>4.906229</td>\n",
              "      <td>...</td>\n",
              "      <td>6.641086</td>\n",
              "      <td>10.535572</td>\n",
              "      <td>19.047827</td>\n",
              "      <td>9.125804</td>\n",
              "      <td>4.227960</td>\n",
              "      <td>4.959233</td>\n",
              "      <td>4.402140</td>\n",
              "      <td>5.035827</td>\n",
              "      <td>1.420060</td>\n",
              "      <td>3.962182e-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>199</td>\n",
              "      <td>5.933960</td>\n",
              "      <td>6.061193</td>\n",
              "      <td>-0.000002</td>\n",
              "      <td>4.902240</td>\n",
              "      <td>5.933982</td>\n",
              "      <td>5.934187</td>\n",
              "      <td>5.939037</td>\n",
              "      <td>5.933995</td>\n",
              "      <td>4.902291</td>\n",
              "      <td>...</td>\n",
              "      <td>6.695330</td>\n",
              "      <td>10.524292</td>\n",
              "      <td>20.248763</td>\n",
              "      <td>9.217610</td>\n",
              "      <td>4.284176</td>\n",
              "      <td>5.247130</td>\n",
              "      <td>4.873523</td>\n",
              "      <td>5.285204</td>\n",
              "      <td>1.412065</td>\n",
              "      <td>1.637207e-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>145</td>\n",
              "      <td>6.110587</td>\n",
              "      <td>6.329464</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>4.908720</td>\n",
              "      <td>6.110644</td>\n",
              "      <td>6.111666</td>\n",
              "      <td>6.111178</td>\n",
              "      <td>6.111119</td>\n",
              "      <td>4.908728</td>\n",
              "      <td>...</td>\n",
              "      <td>7.153246</td>\n",
              "      <td>11.844273</td>\n",
              "      <td>20.321001</td>\n",
              "      <td>10.305776</td>\n",
              "      <td>4.183253</td>\n",
              "      <td>5.556924</td>\n",
              "      <td>5.213365</td>\n",
              "      <td>5.610383</td>\n",
              "      <td>1.487371</td>\n",
              "      <td>-6.668392e-07</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 124 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   abs_max  abs_mean   abs_std  abs_trend       ave  ave_roll_abs_mean_10  \\\n",
              "0      104  5.576567  4.333310  -0.000011  4.884113              5.576566   \n",
              "1      181  5.734167  5.732758  -0.000005  4.725767              5.734187   \n",
              "2      140  6.152647  5.895925   0.000010  4.906393              6.152557   \n",
              "3      199  5.933960  6.061193  -0.000002  4.902240              5.933982   \n",
              "4      145  6.110587  6.329464   0.000005  4.908720              6.110644   \n",
              "\n",
              "   ave_roll_abs_mean_100  ave_roll_abs_mean_1000  ave_roll_abs_mean_50  \\\n",
              "0               5.576655                5.579118              5.576561   \n",
              "1               5.734653                5.739510              5.734444   \n",
              "2               6.153045                6.159850              6.152678   \n",
              "3               5.934187                5.939037              5.933995   \n",
              "4               6.111666                6.111178              6.111119   \n",
              "\n",
              "   ave_roll_mean_10  ...  std_roll_min_10  std_roll_min_100  \\\n",
              "0          4.884088  ...         4.526280          6.522304   \n",
              "1          4.725732  ...         6.398675         10.162552   \n",
              "2          4.906229  ...         6.641086         10.535572   \n",
              "3          4.902291  ...         6.695330         10.524292   \n",
              "4          4.908728  ...         7.153246         11.844273   \n",
              "\n",
              "   std_roll_min_1000  std_roll_min_50  std_roll_std_10  std_roll_std_100  \\\n",
              "0          11.248606         5.787738         2.809071          3.111524   \n",
              "1          21.096014         8.823748         4.120785          4.893431   \n",
              "2          19.047827         9.125804         4.227960          4.959233   \n",
              "3          20.248763         9.217610         4.284176          5.247130   \n",
              "4          20.321001        10.305776         4.183253          5.556924   \n",
              "\n",
              "   std_roll_std_1000  std_roll_std_50  std_to_mean         trend  \n",
              "0           2.769772         3.176148     1.044425 -3.268300e-06  \n",
              "1           4.492905         4.953472     1.394229  9.090424e-07  \n",
              "2           4.402140         5.035827     1.420060  3.962182e-06  \n",
              "3           4.873523         5.285204     1.412065  1.637207e-06  \n",
              "4           5.213365         5.610383     1.487371 -6.668392e-07  \n",
              "\n",
              "[5 rows x 124 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvPlpnL_ALgk",
        "colab_type": "text"
      },
      "source": [
        "#  Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rotmku1tvHcT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import required libraries\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.optimizers import Adam\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import load_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icmZs1mMEEqN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "35af5a03-1557-4084-982a-712fcbbb5a6d"
      },
      "source": [
        "#split the training and target data into 3/4 for training and 1/4 for validation. Choose your split with test_size= values ranging from [0,1] percentage of data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.values, Y.values, test_size=0.25, random_state=42)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3145, 124)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x835SxKuUKFe",
        "colab_type": "text"
      },
      "source": [
        "# Pictorial representation of Neural Network used for Regression\n",
        "![alt text](https://github.com/AshwinDeshpande96/LANL-Earthquake-Prediction/blob/master/regression%20model.jpeg?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUMdqk6tvkuP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We create a Fully Connected Dense Net Initial width 156 and Depth of 5 layers each activated by relu besides the last(output layer) as this is a regression type neural network\n",
        "model = Sequential()\n",
        "model.add(Dense(156 , input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
        "model.add(Dense(128,  kernel_initializer='normal', activation='relu'))\n",
        "model.add(Dense(64,  kernel_initializer='normal', activation='relu'))\n",
        "model.add(Dense(32,  kernel_initializer='normal', activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "model.compile(metrics=['accuracy'],loss='logcosh', optimizer=Adam(lr=1.0e-3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4IWSLzixMKC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "2dcab292-b974-4ac6-d6b7-e62ff200f4ae"
      },
      "source": [
        "filepath = \"/content/gdrive/My Drive/Projects/Kaggle Challenges/LANL Earthquake Prediction/weights.hdf5\"\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min'),                                   #Stop training if no improvement is seen in Validation loss after 10 attempts\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='min'),                    #Slow down Learning rate if Validation Loss is not Decreasing after 5 attempts\n",
        "    ModelCheckpoint(filepath, save_best_only=True,  save_weights_only=False, monitor='val_loss', mode='min') #Model weights are saved in hdf5 file every epoch\n",
        "]\n",
        "#model = load_model('/content/gdrive/My Drive/Projects/Kaggle Challenges/LANL Earthquake Prediction/model.hdf5')\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=64, callbacks=callbacks, verbose=0) #Train Model\n",
        "model.save('/content/gdrive/My Drive/Projects/Kaggle Challenges/LANL Earthquake Prediction/model.hdf5')      #Save Trained model after training"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.00000011116e-07.\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.00000008274e-08.\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.00000008274e-09.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_D6JP8XARyI",
        "colab_type": "text"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEuCAZIvHpG3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e23fec89-bb84-4365-8462-a9613075b151"
      },
      "source": [
        "test_files = [file for file in os.listdir(os.getcwd()) if file.startswith(\"seg\")]                                                       #Get all Test Segments Names for printing name and it's prediction\n",
        "model = load_model('/content/gdrive/My Drive/Projects/Kaggle Challenges/LANL Earthquake Prediction/model.hdf5')                         #Load Trained Model\n",
        "test_df = pd.read_csv(\"/content/gdrive/My Drive/Projects/Kaggle Challenges/LANL Earthquake Prediction/test_set.csv\")                    #Read Test Data\n",
        "print test_df.values.shape                                                                                                              #Check to see if 2624 Rows are present\n",
        "ynew = model.predict(test_df.values)                                                                                                    #Predict Y value('time_to_failure') for ecery 2624 row\n",
        "submission = []                                                                                                                         #Array to save Segment and it's prediction\n",
        "for i in range(len(ynew)):\n",
        "    submission.append( {\"seg_id\":test_files[i], \"time_to_failure\":ynew[i][0]})                                                          #Save every segment and it's prediction in submission array\n",
        "pd.DataFrame(submission).to_csv(\"/content/gdrive/My Drive/Projects/Kaggle Challenges/LANL Earthquake Prediction/submission.csv\", index=False) #Save Predictions in a csv file"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2624, 124)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jcy2g3_bIb8r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "2b53bd74-79dc-4b45-cec5-d0e0e968c838"
      },
      "source": [
        "submission_df = pd.read_csv(\"/content/gdrive/My Drive/Projects/Kaggle Challenges/LANL Earthquake Prediction/submission.csv\")           #Read Saved Submission File\n",
        "print submission_df.head(5)                                                                                                            #Print first 5 predictions of 2624 rows"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           seg_id  time_to_failure\n",
            "0  seg_63c8e4.csv         9.291997\n",
            "1  seg_5f352e.csv         2.569135\n",
            "2  seg_8fc754.csv         5.890961\n",
            "3  seg_f7290f.csv         2.364862\n",
            "4  seg_2383c2.csv         4.226511\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poKFNqLCAmoo",
        "colab_type": "text"
      },
      "source": [
        "# Trials"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A5_nch8xllB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "# evaluate model with standardized dataset\n",
        "estimator = KerasRegressor(build_fn=model, epochs=100, batch_size=5, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6nl7GJ4bohP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kfold = KFold(n_splits=10, random_state=seed)\n",
        "results = cross_val_score(estimator, X_train, target, cv=kfold)\n",
        "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIdzGAGdxfwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluate model with standardized dataset\n",
        "np.random.seed(seed)\n",
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasRegressor(build_fn=model, epochs=50, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = KFold(n_splits=10, random_state=seed)\n",
        "results = cross_val_score(pipeline, X_train, target, cv=kfold)\n",
        "print(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}